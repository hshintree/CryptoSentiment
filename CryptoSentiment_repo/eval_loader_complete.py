#!/usr/bin/env python3
"""
Complete EvalLoader using the enhanced EWMA MarketLabeler.

This is the final, production-ready version that implements all requirements:
- EWMA volatility-adjusted barriers: Uâ‚œ = Pâ‚œ + Pâ‚œÂ·Ïƒâ‚œÂ·Fáµ¤, Lâ‚œ = Pâ‚œ â€“ Pâ‚œÂ·Ïƒâ‚œÂ·Fâ‚—
- 2-day minimum trend enforcement
- Risk-adjusted Sharpe with 4% risk-free rate
- Individual tweet labeling for paper compliance
- Proper intra-day data handling with unique trading days
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from market_labeler_ewma import MarketLabelerEWMA
from pathlib import Path


class EvalLoaderComplete:
    def __init__(self, csv_path, config_path="config_ewma.yaml"):
        """
        Initialize Complete EvalLoader with enhanced EWMA market labeling.
        
        Args:
            csv_path: Path to combined_dataset_raw.csv
            config_path: Path to config_ewma.yaml for enhanced labeler
        """
        self.csv_path = csv_path
        self.config_path = config_path
        self.labeler = MarketLabelerEWMA(config_path)
        
        # Load and clean the dataset
        print(f"Loading dataset from: {csv_path}")
        self.df = self._load_and_clean_dataset()
        print(f"Loaded {len(self.df)} rows. Date range: {self.df['date'].min()} to {self.df['date'].max()}")

    def _load_and_clean_dataset(self):
        """Load the CSV and handle date parsing issues robustly."""
        df = pd.read_csv(self.csv_path)
        
        # Handle date parsing with mixed formats
        print("Parsing dates...")
        df['date'] = pd.to_datetime(df['date'], format='mixed', errors='coerce')
        
        # Drop rows with invalid dates
        before_drop = len(df)
        df = df.dropna(subset=['date'])
        if len(df) < before_drop:
            print(f"Dropped {before_drop - len(df)} rows with invalid dates")
        
        # Ensure required columns exist
        required_cols = ['date', 'Tweet Content', 'Close', 'Is_Event']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")
        
        # Sort by date
        df = df.sort_values('date').reset_index(drop=True)
        
        return df

    def create_eval_datasets(self, n_folds=5):
        """
        Create complete evaluation datasets with enhanced EWMA labeling.
        
        Returns:
            Dictionary with ea_train_folds, ea_test_folds, eb_train_folds, eb_test_folds,
            ea_full, eb_full, and all_data
        """
        print("="*70)
        print("CREATING COMPLETE EVALUATION DATASETS (EWMA-ENHANCED)")
        print("="*70)
        
        # 1. Apply enhanced EWMA market labeling to full dataset
        print("Applying enhanced EWMA market labeling to all tweets...")
        print("Features: EWMA volatility, 2-day minimum, risk-adjusted Sharpe")
        df_labeled = self.labeler.label_data(self.df)
        
        print(f"\nğŸ“Š ENHANCED LABELING RESULTS:")
        label_counts = df_labeled['Label'].value_counts()
        print(label_counts)
        label_pcts = (label_counts / len(df_labeled) * 100).round(2)
        print(f"Percentages: {label_pcts.to_dict()}")
        
        # Show volatility statistics
        print(f"\nğŸ“ˆ VOLATILITY STATISTICS:")
        print(f"Mean EWMA volatility: {df_labeled['Volatility'].mean():.4f}")
        print(f"Volatility range: [{df_labeled['Volatility'].min():.4f}, {df_labeled['Volatility'].max():.4f}]")
        
        # 2. Create Ea dataset (2020, ~60k tweets evenly distributed)
        print("\n" + "="*50)
        print("CREATING EA DATASET (2020 FINE-TUNING)")
        print("="*50)
        ea_df = self._create_ea_dataset(df_labeled)
        print(f"âœ… Ea dataset: {len(ea_df)} tweets")
        print(f"âœ… Ea label distribution: {ea_df['Label'].value_counts().to_dict()}")
        
        # 3. Create Eb dataset (2015-2019 & 2021-2023, ~40k tweets event-focused)
        print("\n" + "="*50)
        print("CREATING EB DATASET (EVENT EVALUATION)")
        print("="*50)
        eb_df = self._create_eb_dataset(df_labeled)
        print(f"âœ… Eb dataset: {len(eb_df)} tweets")
        print(f"âœ… Eb label distribution: {eb_df['Label'].value_counts().to_dict()}")
        print(f"âœ… Eb event distribution: {eb_df['Is_Event'].value_counts().to_dict()}")
        
        # 4. Time-series splits (grouped by temporal proximity)
        print("\n" + "="*50)
        print("CREATING GROUPED TIME-SERIES SPLITS")
        print("="*50)
        ea_train_folds, ea_test_folds = self._create_time_folds(ea_df, n_folds)
        eb_train_folds, eb_test_folds = self._create_time_folds(eb_df, n_folds)
        
        print(f"âœ… Ea fold sizes - train: {[len(f) for f in ea_train_folds]}")
        print(f"âœ… Ea fold sizes - test: {[len(f) for f in ea_test_folds]}")
        print(f"âœ… Eb fold sizes - train: {[len(f) for f in eb_train_folds]}")
        print(f"âœ… Eb fold sizes - test: {[len(f) for f in eb_test_folds]}")
        
        # 5. Daily aggregation for final model training
        print("\n" + "="*50)
        print("CREATING DAILY AGGREGATED DATASET")
        print("="*50)
        all_data = self._aggregate_all_data([ea_df, eb_df])
        print(f"âœ… All data (daily aggregated): {len(all_data)} days")
        
        # 6. Final validation and paper compliance check
        self._validate_paper_compliance(ea_df, eb_df, all_data)
        
        return {
            "ea_train_folds": ea_train_folds,
            "ea_test_folds":  ea_test_folds,
            "eb_train_folds": eb_train_folds,
            "eb_test_folds":  eb_test_folds,
            "ea_full":        ea_df,
            "eb_full":        eb_df,
            "all_data":       all_data
        }

    def _create_ea_dataset(self, df_labeled):
        """
        Create Ea dataset: 2020 data with ~60k tweets evenly distributed across labels.
        Target: 20k each of Bullish/Bearish/Neutral (or proportional if insufficient).
        """
        # Filter for 2020
        ea = df_labeled[df_labeled['date'].dt.year == 2020].copy()
        print(f"2020 data available: {len(ea)} tweets")
        
        # Drop rows with missing labels
        ea = ea[ea['Label'].isin(['Bullish', 'Bearish', 'Neutral'])]
        print(f"After label filtering: {len(ea)} tweets")
        
        # Check available counts
        label_counts = ea['Label'].value_counts()
        print(f"Available labels in 2020: {label_counts.to_dict()}")
        
        # Determine sampling strategy based on availability
        total_target = 60000
        min_available = label_counts.min()
        
        if min_available * 3 >= total_target:
            # We can achieve perfect balance
            target_per_class = total_target // 3
            print(f"Perfect balance achievable: {target_per_class} per class")
        else:
            # Take all of minority class and balance others
            target_per_class = min(min_available, 20000)
            print(f"Limited by minority class: {target_per_class} per class")
        
        # Sample each class
        sampled_dfs = []
        for label in ['Bullish', 'Bearish', 'Neutral']:
            group = ea[ea['Label'] == label]
            n_available = len(group)
            n_target = min(target_per_class, n_available)
            
            if n_available > 0:
                if n_available >= n_target:
                    sampled = group.sample(n=n_target, random_state=42)
                else:
                    sampled = group  # Take all available
                sampled_dfs.append(sampled)
                print(f"  {label}: {len(sampled)} tweets (target: {n_target}, available: {n_available})")
            else:
                print(f"  {label}: 0 tweets available")
        
        if not sampled_dfs:
            raise ValueError("No valid tweets found for Ea dataset")
        
        ea_sampled = pd.concat(sampled_dfs, ignore_index=True)
        ea_sampled = ea_sampled.sort_values('date').reset_index(drop=True)
        
        # Add Previous_Label
        ea_sampled = self._add_previous_day_label(ea_sampled)
        
        return ea_sampled

    def _create_eb_dataset(self, df_labeled):
        """
        Create Eb dataset: 2015-2019 & 2021-2023 data, event-focused.
        Target: up to 25k event tweets + fill to ~40k with non-event tweets.
        """
        # Filter for 2015-2019 & 2021-2023 (excluding 2020)
        eb = df_labeled[
            ((df_labeled['date'].dt.year >= 2015) & (df_labeled['date'].dt.year <= 2019)) |
            ((df_labeled['date'].dt.year >= 2021) & (df_labeled['date'].dt.year <= 2023))
        ].copy()
        print(f"2015-2019 & 2021-2023 data available: {len(eb)} tweets")
        
        # Ensure Is_Event is numeric
        eb['Is_Event'] = pd.to_numeric(eb['Is_Event'], errors='coerce').fillna(0)
        
        # Separate event and non-event tweets
        event_tweets = eb[eb['Is_Event'] == 1]
        non_event_tweets = eb[eb['Is_Event'] == 0]
        
        print(f"Event tweets available: {len(event_tweets)}")
        print(f"Non-event tweets available: {len(non_event_tweets)}")
        
        # Sample up to 25k event tweets (prioritize event tweets)
        n_event_target = min(25000, len(event_tweets))
        if len(event_tweets) > 0:
            if len(event_tweets) >= n_event_target:
                event_sample = event_tweets.sample(n=n_event_target, random_state=42)
            else:
                event_sample = event_tweets
        else:
            event_sample = pd.DataFrame(columns=eb.columns)
        
        print(f"Sampled event tweets: {len(event_sample)}")
        
        # Fill up to ~40k total with non-event tweets
        n_non_event_target = max(0, min(40000 - len(event_sample), len(non_event_tweets)))
        if n_non_event_target > 0 and len(non_event_tweets) > 0:
            if len(non_event_tweets) >= n_non_event_target:
                non_event_sample = non_event_tweets.sample(n=n_non_event_target, random_state=42)
            else:
                non_event_sample = non_event_tweets
        else:
            non_event_sample = pd.DataFrame(columns=eb.columns)
        
        print(f"Sampled non-event tweets: {len(non_event_sample)}")
        
        # Combine and sort
        eb_sampled = pd.concat([event_sample, non_event_sample], ignore_index=True)
        eb_sampled = eb_sampled.sort_values('date').reset_index(drop=True)
        
        # Add Previous_Label
        eb_sampled = self._add_previous_day_label(eb_sampled)
        
        return eb_sampled

    def _create_time_folds(self, df, n_folds):
        """Create time-series splits using sklearn.model_selection.TimeSeriesSplit."""
        tscv = TimeSeriesSplit(n_splits=n_folds)
        train_folds, test_folds = [], []
        
        df_sorted = df.sort_values('date').reset_index(drop=True)
        
        for train_idx, test_idx in tscv.split(df_sorted):
            train_fold = df_sorted.iloc[train_idx].copy()
            test_fold = df_sorted.iloc[test_idx].copy()
            train_folds.append(train_fold)
            test_folds.append(test_fold)
        
        return train_folds, test_folds

    def _add_previous_day_label(self, df):
        """Add Previous_Label column with the previous day's label."""
        df_with_prev = df.sort_values('date').reset_index(drop=True)
        df_with_prev['Previous_Label'] = df_with_prev['Label'].shift(1).fillna('Neutral')
        return df_with_prev

    def _aggregate_all_data(self, dfs):
        """
        Concatenate Ea and Eb, then aggregate by date:
        - majority vote for Label
        - mean for numeric features  
        - concatenate tweets with [SEP]
        - max for Is_Event
        - add Previous_Label
        """
        # Concatenate all datasets
        all_df = pd.concat(dfs, ignore_index=True)
        all_df = all_df.sort_values('date')
        
        # Group by date (date only, not datetime)
        all_df['date_only'] = all_df['date'].dt.date
        
        def majority_vote(series):
            """Return the most common value, or first if tie."""
            mode_result = series.mode()
            return mode_result.iloc[0] if len(mode_result) > 0 else series.iloc[0]
        
        def concatenate_tweets(series):
            """Concatenate tweets with [SEP] separator."""
            return ' [SEP] '.join(series.astype(str))
        
        # Define aggregation functions
        agg_dict = {
            'Label': majority_vote,
            'Tweet Content': concatenate_tweets,
            'Is_Event': 'max',
            'Close': 'mean',
            'Volatility': 'mean'
        }
        
        # Add other numeric columns if they exist
        numeric_cols = all_df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col not in agg_dict and col not in ['Is_Event']:
                agg_dict[col] = 'mean'
        
        # Perform aggregation
        grouped = all_df.groupby('date_only').agg(agg_dict).reset_index()
        
        # Convert date_only back to datetime
        grouped['date'] = pd.to_datetime(grouped['date_only'])
        grouped = grouped.drop(columns=['date_only'])
        
        # Sort by date and add Previous_Label
        grouped = grouped.sort_values('date').reset_index(drop=True)
        grouped = self._add_previous_day_label(grouped)
        
        return grouped

    def _validate_paper_compliance(self, ea_df, eb_df, all_data):
        """Validate that datasets meet paper requirements."""
        print("\n" + "="*70)
        print("ğŸ“‹ PAPER COMPLIANCE VALIDATION")
        print("="*70)
        
        # Size validation
        ea_size = len(ea_df)
        eb_size = len(eb_df)
        print(f"âœ… Ea size: {ea_size:,} tweets (target: ~60,000)")
        print(f"âœ… Eb size: {eb_size:,} tweets (target: ~40,000)")
        
        # Balance validation for Ea
        ea_balance = ea_df['Label'].value_counts()
        ea_min, ea_max = ea_balance.min(), ea_balance.max()
        balance_ratio = ea_min / ea_max if ea_max > 0 else 0
        print(f"âœ… Ea balance ratio: {balance_ratio:.3f} (target: ~0.33 for even distribution)")
        
        # Event focus validation for Eb
        eb_events = eb_df['Is_Event'].sum()
        event_pct = (eb_events / len(eb_df) * 100) if len(eb_df) > 0 else 0
        print(f"âœ… Eb event tweets: {eb_events:,} ({event_pct:.1f}% of Eb dataset)")
        
        # Temporal coverage
        ea_date_range = f"{ea_df['date'].min().date()} to {ea_df['date'].max().date()}"
        eb_date_range = f"{eb_df['date'].min().date()} to {eb_df['date'].max().date()}"
        print(f"âœ… Ea temporal coverage: {ea_date_range} (2020 only)")
        print(f"âœ… Eb temporal coverage: {eb_date_range} (excludes 2020)")
        
        # Enhanced labeling validation
        non_neutral_ea = (ea_df['Label'] != 'Neutral').sum()
        non_neutral_eb = (eb_df['Label'] != 'Neutral').sum()
        print(f"âœ… Ea actionable signals: {non_neutral_ea:,} ({non_neutral_ea/len(ea_df)*100:.1f}%)")
        print(f"âœ… Eb actionable signals: {non_neutral_eb:,} ({non_neutral_eb/len(eb_df)*100:.1f}%)")
        
        print(f"\nğŸ¯ ENHANCED FEATURES IMPLEMENTED:")
        print(f"âœ… EWMA volatility-adjusted barriers")
        print(f"âœ… 2-day minimum trend enforcement")
        print(f"âœ… Risk-adjusted Sharpe optimization")
        print(f"âœ… Individual tweet labeling maintained")
        print(f"âœ… Proper intra-day data handling")
        
        print(f"\nğŸ”¥ READY FOR PAPER IMPLEMENTATION!")


def main():
    """Main function to test the complete EvalLoader."""
    csv_path = "data/combined_dataset_raw.csv"
    
    if not Path(csv_path).exists():
        print(f"Error: {csv_path} not found")
        return
    
    try:
        loader = EvalLoaderComplete(csv_path, config_path="config_ewma.yaml")
        result = loader.create_eval_datasets(n_folds=5)
        
        print("\n" + "="*70)
        print("ğŸ‰ COMPLETE EVALUATION DATASETS SUMMARY")
        print("="*70)
        
        print(f"ğŸ“Š Dataset Sizes:")
        print(f"  Ea full: {result['ea_full'].shape}")
        print(f"  Eb full: {result['eb_full'].shape}")
        print(f"  All data (aggregated): {result['all_data'].shape}")
        
        print(f"\nğŸ“Š Final Label Distributions:")
        print(f"  Ea (2020 training): {result['ea_full']['Label'].value_counts().to_dict()}")
        print(f"  Eb (event evaluation): {result['eb_full']['Label'].value_counts().to_dict()}")
        
        print(f"\nğŸ“Š Cross-Validation Folds:")
        print(f"  Ea train folds: {[len(f) for f in result['ea_train_folds']]}")
        print(f"  Ea test folds: {[len(f) for f in result['ea_test_folds']]}")
        print(f"  Eb train folds: {[len(f) for f in result['eb_train_folds']]}")
        print(f"  Eb test folds: {[len(f) for f in result['eb_test_folds']]}")
        
        print(f"\nğŸ“ Sample Enhanced Data:")
        sample_cols = ['date', 'Label', 'Volatility', 'Upper Barrier', 'Lower Barrier', 'Tweet Content']
        print("Ea sample (with EWMA features):")
        print(result['ea_full'][sample_cols].head(3))
        
        print("\nEb sample (event-focused):")
        print(result['eb_full'][['date', 'Label', 'Is_Event', 'Volatility', 'Tweet Content']].head(3))
        
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 