#!/usr/bin/env python3
"""
Full 5-fold CV on **Ea** (year 2020, 36â€“60 k balanced tweets),
then out-of-sample evaluation on **Eb** (â‰ˆ40 k event-sampled tweets
from 2015-2019 & 2021-2023, paper-exact configuration).

â€¢ strict temporal separation (Ea 2020 âŸ‚ Eb â‰ 2020)  
â€¢ per-fold preprocessing & EWMA-TBL labelling to avoid leakage  
â€¢ single "best-epoch" checkpoint per fold
"""

import yaml
import pandas as pd
from pathlib import Path
from datetime import datetime

from preprocessor import Preprocessor
from market_labeler_ewma import MarketLabelerTBL
from model import Model
from trainer import Trainer, cross_val_predict
from sklearn.metrics import (accuracy_score,
                             precision_recall_fscore_support)
from glob import glob

# â”€â”€ 1. Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€ paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CFG        = "config.yaml"
EA_CSV     = Path("data/#1train.csv")          # Ea (training)
EB_CSV     = Path("data/#2val.csv")            # Eb (evaluation)

def strip_leaks(df: pd.DataFrame) -> pd.DataFrame:
    """Remove leaky columns that would cause data leakage."""
    leaky = [
        "Upper Barrier", "Lower Barrier", "Vertical Barrier",
        "Volatility", "Previous Label", "Previous_Label"
    ]
    return df.drop(columns=[c for c in leaky if c in df.columns])

# â”€â”€ 1.5. Load EA data (2020 training) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("="*70)
print("ğŸš€ FULL CV RUN: EA TRAINING (2020) + EB EVALUATION (â‰ 2020)")
print("="*70)

print("\nğŸ“ Loading EA dataset  (#1train.csv  â€“ 2020 only)â€¦")
# parse_dates can silently fail â†’ coerce to datetime explicitly
ea = pd.read_csv(EA_CSV)
ea = ea[pd.to_datetime(ea["date"], errors="coerce").notna()]  # drop NaT rows
ea = ea.assign(date=lambda d: pd.to_datetime(d["date"], errors="coerce"))
ea = strip_leaks(ea).rename(columns={"date": "Tweet Date"})
# guarantee the dtype
ea["Tweet Date"] = pd.to_datetime(ea["Tweet Date"], errors="coerce")
print(f"EA columns after strip: {ea.columns.tolist()}")
print(f"EA rows: {len(ea):,} tweets")

# â”€â”€ 1.6  Load EB (event evaluation) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ“ Loading EB dataset  (#2val.csv â€“ event evaluation)â€¦")
eb = pd.read_csv(EB_CSV)
# date cleaning
eb = eb[pd.to_datetime(eb["date"], errors="coerce").notna()]
eb = eb.assign(date=lambda d: pd.to_datetime(d["date"], errors="coerce"))
eb = strip_leaks(eb).rename(columns={"date": "Tweet Date"})
eb["Tweet Date"] = pd.to_datetime(eb["Tweet Date"], errors="coerce")
print(f"EB  columns after strip: {eb.columns.tolist()}")
print(f"EB  rows: {len(eb):,} tweets")

# Verify temporal separation
ea_years = sorted(ea['Tweet Date'].dt.year.unique())
eb_years  = sorted(eb['Tweet Date'].dt.year.unique())
print(f"\nğŸ” Temporal verification:")
print(f"  EA years: {ea_years} (training)")
print(f"  EB  years: {eb_years} (evaluation)")

overlap = set(ea_years) & set(eb_years)
if overlap:
    print(f"âŒ ERROR: Year overlap detected: {overlap}")
    exit(1)
else:
    print("âœ… No temporal overlap - proper out-of-sample setup")

# â”€â”€ 2. Check ground-truth labels are preserved â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\nğŸ” Ground-truth label verification:")
if 'Label' in ea.columns:
    ea_label_dist = ea['Label'].value_counts()
    print(f"  EA label distribution: {dict(ea_label_dist)}")
else:
    print("  âŒ EA missing 'Label' column!")

if 'Label' in eb.columns:
    eb_label_dist = eb['Label'].value_counts()
    print(f"  EB  label distribution:  {dict(eb_label_dist)}")
else:
    print("  âŒ EB missing 'Label' column!")

# â”€â”€ 3. Instantiate model & trainer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\nâš™ï¸  Setting up model and trainer...")
with open(CFG) as f:
    cfg = yaml.safe_load(f)
model_cfg = cfg["model"]
model     = Model(model_cfg)
trainer   = Trainer(model, ea, CFG)  # Pass EA data, preprocessing happens per-fold
print(f"Using device: {trainer.device}")

# quick knobs before trainer.train()
trainer.epochs        = 2          # â† from 2 â†’ 3
trainer.learning_rate = 1e-5       
# If you switch to prompt-tuning / last-layer-only:
# trainer.learning_rate = 8e-5

print(f"\nğŸ’¡ Training configuration:")
print(f"  Batch size: {trainer.batch_size}")
print(f"  Epochs: {trainer.epochs}")
print(f"  Learning rate: {trainer.learning_rate}")
print(f"  Prompt tuning: {model.use_prompt_tuning}")
print(f"\nğŸ“ˆ Expected progression:")
print(f"  Training loss: ~0.9 â†’ 0.6 over 3 epochs")
print(f"  Validation accuracy: 45-55% (realistic for small Ea subset)")
print(f"  Each fold will have similar calendar length in train vs val")

# â”€â”€ 4. Train on EA dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n" + "="*70)
print(f"ğŸ¯ STARTING 5-FOLD CV TRAINING ON EA (2020)")
print(f"="*70)
print(f"âš¡ Training on {len(ea):,} EA tweets...")
print("ğŸ’¾ Memory Management: Models moved to CPU after each fold to free MPS memory")
print("ğŸ”’ Leak Prevention: Preprocessing and labeling done per-fold")
print("ğŸ“Š Expected realistic metrics: 40-85% validation accuracy")
print("âš ï¸  If you see 95%+ accuracy, there's still label leakage!")

trainer.train()

# â”€â”€ 5. Save per-fold checkpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\nğŸ’¾ Saving model checkpoints...")
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
base_dir  = Path("models") / f"fixed?_{timestamp}"
for fold_idx, fold_state in enumerate(trainer.fold_states, start=1):
    ckpt_dir = base_dir / f"fold{fold_idx}_epoch{trainer.epochs}"
    ckpt_dir.mkdir(parents=True, exist_ok=True)
    # Save the fine-tuned model & tokenizer for this fold (models are on CPU)
    fold_state["model"].bert_model.save_pretrained(ckpt_dir)
    fold_state["model"].tokenizer.save_pretrained(ckpt_dir)
    print(f"âœ“ Saved fold {fold_idx} checkpoint to {ckpt_dir}")

# â”€â”€ 6. Predictions on the EB event-evaluation split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n" + "="*70)
print(f"ğŸ”® OUT-OF-SAMPLE EVALUATION ON EB (events 2015-19 & 2021-23)")
print(f"="*70)

if trainer.fold_states:
    print("ğŸ”® Generating predictions on EB evaluation setâ€¦")
    
    # Preprocess and label EB data for prediction
    print("  ğŸ“Š Preprocessing EB data...")
    from preprocessor import Preprocessor
    pred_preprocessor = Preprocessor(CFG)
    pred_preprocessor.fit(ea)           # fit on training only
    eb_prep  = pred_preprocessor.transform(eb.copy())
    
    # âš ï¸  Do NOT relabel Eb â€“ keep its ground-truth labels.
    eb_prep  = eb_prep.rename(columns={"date": "Tweet Date"})
    # add causal Previous-Label feature once (uses past values only)
    from market_labeler_ewma import MarketFeatureGenerator
    feat_gen = MarketFeatureGenerator(CFG)
    feat_gen.fit(eb_prep)
    eb_prep["Previous Label"] = feat_gen.transform(eb_prep)
    
    print(f"  ğŸ¯ Running cross-validation prediction on {len(eb_prep):,} EB tweetsâ€¦")
    sig_eb = cross_val_predict(trainer, eb_prep)
    
    # Save per-tweet predictions
    sig_eb.to_csv("signals_eb.csv", index=False)
    print("âœ“ Saved per-tweet predictions â†’ signals_eb.csv")
    
    # Generate daily signals via majority vote
    print("  ğŸ“… Aggregating daily signals...")
    daily_eb = (
        sig_eb.groupby("Tweet Date").Pred_Label
        .agg(lambda x: x.value_counts().idxmax())
        .sort_index()
        .rename("Daily_Signal")
    )
    daily_eb.to_csv("eb_daily_signals.csv")
    print("âœ“ Saved per-day signals     â†’ eb_daily_signals.csv")
    
    # Summary statistics
    eb_pred_dist    = sig_eb['Pred_Label'].value_counts()
    daily_pred_dist = daily_eb.value_counts()
    print(f"\nğŸ“Š EB Prediction Summary:")
    print(f"  Per-tweet predictions: {dict(eb_pred_dist)}")
    print(f"  Daily signals:        {dict(daily_pred_dist)}")
    
    # Performance hint
    if 'Label' in sig_eb.columns:
        # --- core metrics -------------------------------------------------
        y_true = sig_eb['Label'].map({'Bearish':0,'Neutral':1,'Bullish':2}).values
        y_pred = sig_eb['Pred_Label'].map({'Bearish':0,'Neutral':1,'Bullish':2}).values

        acc  = accuracy_score(y_true, y_pred)
        prec, rec, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average="macro", zero_division=0)

        print(f"  Accuracy : {acc:.3f}")
        print(f"  Precision: {prec:.3f}")
        print(f"  Recall   : {rec:.3f}")
        print(f"  F1-score : {f1:.3f}")

        # --------- quick leakage sanity check ----------------------------
        if acc > 0.90:
            print("  âš ï¸  Very high accuracy - check for data leakage!")
        elif acc > 0.60:
            print("  âœ… Good performance on out-of-sample data")
        else:
            print("  ğŸ“ˆ Moderate performance - may need model improvements")
else:
    print("âš ï¸  No trained models â€“ skipping EB inference")

print(f"\n" + "="*70)
print(f"ğŸ‰ FULL CV RUN COMPLETED!")
print(f"="*70)
print(f"ğŸ“ Files generated:")
print(f"  â€¢ Model checkpoints: models/ea_2019-2020_{timestamp}/")
print(f"  â€¢ EB  predictions: signals_eb.csv")
print(f"  â€¢ Daily signals:   eb_daily_signals.csv")
print(f"  â€¢ Training metrics: training_metrics.json")
print(f"\nâœ… Ready for paper results analysis!") 